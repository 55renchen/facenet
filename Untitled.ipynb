{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models import FaceNetModel\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.optim import Adam\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LfwMultiImages(Dataset):\n",
    "    def __init__(self, trfrm, root='/Users/khairulimam/workspaces/DeepLearning/datasets/lfw-mtcnn-182/', min_images=6):\n",
    "        super().__init__()\n",
    "        self.root = root\n",
    "        self.min_images = min_images\n",
    "        dataset = ImageFolder(root, transform=trfrm)\n",
    "        self.idx_classes = dataset.class_to_idx.values()\n",
    "        self.classes_only = [i[1] for i in dataset]\n",
    "        self.class_multi_images = list(filter(lambda i: self.classes_only.count(i) > min_images, self.idx_classes))\n",
    "        self.dataset_multi_images = list(filter(lambda i: i[1] in self.class_multi_images, dataset))\n",
    "    def __getitem__(self, index):\n",
    "        return self.dataset_multi_images[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset_multi_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfw_multi_images = LfwMultiImages(trfrm, min_images=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "530"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lfw_multi_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9953])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cosine_similarity(embed1, embed2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9704])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.pairwise_distance(embed1, embed2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(lfw_multi_images, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "state2 = torch.load('log/last_checkpoint.pth', map_location='cpu')\n",
    "state1 = torch.load('log/best_state.pth', map_location='cpu')\n",
    "state3 = torch.load('log/best_state_92.pth', map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = FaceNetModel(embedding_size=128, num_classes=500)\n",
    "model1.load_state_dict(state1['state_dict'])\n",
    "model2 = FaceNetModel(embedding_size=128, num_classes=500)\n",
    "model2.load_state_dict(state2['state_dict'])\n",
    "model3 = FaceNetModel(embedding_size=128, num_classes=500)\n",
    "model3.load_state_dict(state3['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trfrm =transforms.Compose([\n",
    "            transforms.Resize(224),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])])\n",
    "topil = transforms.ToPILImage()\n",
    "totensor = transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.eval()\n",
    "with torch.no_grad():\n",
    "    images, labels = next(iter(dataloader))\n",
    "    embeds = model1.forward(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9207000000000001"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state2['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeds[0].view(1, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "img0, l0 = lfw_multi_images[0]\n",
    "img1, l1 = lfw_multi_images[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1871"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.0152, -2.0152, -2.0152,  ..., -2.1008, -2.0837, -2.0665],\n",
       "         [-2.0152, -2.0152, -2.0152,  ..., -2.0837, -2.0665, -2.0665],\n",
       "         [-2.0152, -2.0152, -2.0152,  ..., -2.0837, -2.0665, -2.0665],\n",
       "         ...,\n",
       "         [-0.6281, -0.4397, -0.1828,  ..., -1.0562, -0.9877, -0.9534],\n",
       "         [-0.9534, -0.7993, -0.5767,  ..., -1.0390, -1.0219, -1.0219],\n",
       "         [-1.1589, -1.0562, -0.9192,  ..., -1.0390, -1.0733, -1.0904]],\n",
       "\n",
       "        [[-2.0007, -2.0007, -2.0007,  ..., -2.0007, -1.9832, -1.9657],\n",
       "         [-2.0007, -2.0007, -2.0007,  ..., -1.9832, -1.9657, -1.9657],\n",
       "         [-2.0007, -2.0007, -2.0182,  ..., -1.9832, -1.9657, -1.9657],\n",
       "         ...,\n",
       "         [-0.4776, -0.2850, -0.0049,  ..., -0.7402, -0.6702, -0.6352],\n",
       "         [-0.7927, -0.6352, -0.4076,  ..., -0.7227, -0.7052, -0.7052],\n",
       "         [-0.9678, -0.8627, -0.7227,  ..., -0.7227, -0.7577, -0.7752]],\n",
       "\n",
       "        [[-0.0790, -0.0790, -0.0790,  ...,  0.1302,  0.1476,  0.1651],\n",
       "         [-0.0790, -0.0790, -0.0790,  ...,  0.1476,  0.1651,  0.1651],\n",
       "         [-0.0790, -0.0790, -0.0615,  ...,  0.1302,  0.1476,  0.1476],\n",
       "         ...,\n",
       "         [ 0.0953,  0.2871,  0.5659,  ..., -0.1138, -0.0441, -0.0092],\n",
       "         [-0.2010, -0.0441,  0.1825,  ..., -0.0615, -0.0615, -0.0615],\n",
       "         [-0.3927, -0.2881, -0.1312,  ..., -0.0441, -0.0790, -0.0964]]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.8050]), tensor([0.9968]))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.eval()\n",
    "with torch.no_grad():\n",
    "    embed1 = model2.forward(img0.unsqueeze(0))\n",
    "    embed2 = model2.forward(img1.unsqueeze(0))\n",
    "F.pairwise_distance(embed1, embed2), F.cosine_similarity(embed1, embed2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.3532])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.pairwise_distance(embed1, embed2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'state_dict' in state.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(state['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = Adam(model.module.parameters(), lr=0.001)\n",
    "scheduler = lr_scheduler.StepLR(optim, step_size=50, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 1e-05\n"
     ]
    }
   ],
   "source": [
    "for _ in range(85):\n",
    "    scheduler.step()\n",
    "    if scheduler.last_epoch % scheduler.step_size == 0:\n",
    "        print(\"LR:\", ', '.join(map(str, scheduler.get_lr())))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
